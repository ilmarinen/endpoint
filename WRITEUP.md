Based on the breadth and depth of the task, I made some simplifying assumptions:

1. I generously interpreted the fact that the site held exactly one page, to mean
   that I was allowed to serve up a templated page, and that I did not have to
   store the page contents in a data model.

2. I prioritized flexibility and scalability. Flexibility in the sense that, the
   delivered result is admittedly a bit rough around the edges, but that if there
   was focused effort, it could be easily brought up to something production ready
   in short order, i.e. the components and technologies used are flexible enough
   to be moulded to and secured within a production environment. And I prioritized
   scalability in the sense that I chose components that have been demonstrated
   to work at scale, albiet with significantly different configurations and
   deployment architectures.

3. Security took a back seat as I needed to be able to troubleshoot things as some
   of these components were fairly new for me to use. So for example I needed to
   have Elasticsearch listening on the `0.0.0.0` interface so that I could browse
   the indexes and check their mappings etc. The same goes for Kibana.

4. I assumed that I could use one of my existing toy projects `Endpoint` in order
   to speed up development. This is an existing project to which I am almost the sole
   contributor (there were a few commits from a friend a while ago). The reason that
   I felt ok doing this is that it is effectively equivalent to using an existing
   framework. But this is my hand rolled framework, so I know it pretty well.


Basic Stack
------------

1. I used the ELK (Elasticsearch, Logstash, Filebeat and Kibana) stack for logging and
   visualization.

2. The application server was repurposed from my existing Endpoint project, written
   in Python 2.7 and using Flask.

3. The webserver used was Nginx, which served up the Flask app using uWsgi.

4. The Javascript code was written using React.


Overall Strategy
-----------------

My overarching strategy was simple. I created three REST endpoints in my application
server:

1. `/api/v1/event//initial/<uuid-token>`
   The UI hit this once each time the Latest Plane Crash page was loaded.

2. `/api/v1/event/<uuid-token>`
   The UI polled this every 500ms oncwe the Latest Plane Crash page was loaded.

3. `/api/v1/event/interesting/<uuid-token>`
   The UI hit this once if a user clicked on the `Informative` button.

Each time a user loaded the Latest Plane Crash page, a unique (with certain very high
probability) UUID4 token is generated by the user and passed into all requests to the
events api above. This helps to keep track of users between their dropped web connections.

Then I just made sure to ingest the Nginx logs (since all requests to the uWsgi app went through
Nginx) into Elasticsearch and index the `url` field.

This allowed me to run queries and make visualizations in Kibana.


Deployment
-----------

I used Ubuntu 14.04 LTS on DigitalOcean. For setting up the ELK stack I used
this (https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04)
tutorial as a guide. And then modified it in order to ingest Nginx logs and
index the appropriate fields in Elasticsearch so that they could be made
searchable from Kibana.

Filebeat was configured to ingest the Nginx logs, parse them and feed them into Logstash.
Logstash parsed the logs and fed them into Elasticsearch. Kibana was hosted on the
same server and connected to Elasticsearch in order to run queries and build
visualizations.

For this deployment Elasticsearch, Filebeat, Logstash, Kibana, Nginx and the application
server were all hosted on the same server. However in practice, these could be split
up to aid with scalability. Namely, we could have Elasticsearch reside on its own server,
or we could deploy an Elasticsearch cluster. We could have multiple instances consisting
of Nginx + Application Server behind a load balancer (we could use a separate Nginx instance
for this, or even HAProxy). Filebeat would need to run on each Nginx + Application instance
as well as on the load balancer. And logstash could reside on its own instance.

One benefit ot not logging at the application server level, and instead relying on the
Nginx server logs, is that it gives us the option to use Nginx caching to greatly
increase our ability to field requests, while still preserving our ability log all the
events we need.


Source Code
------------

The main application server Python code resides under the `./endpoint` folder in the repo.
There are three main packages (only two of which are relevant for this project).

1. `public`
   The route for serving up the Latest Plane Crash Page

2. `webevent`
   The REST api routes for fielding the various events from the UI.

3. `user`
   A set of models and routes for managing access control. It was too baked in to the
   application for me to easily remove it.

The Jinja2 tempalted HTML for the Latest Plane Crash page is stored under
`./endpoint/templates/index.htnl`. When you examine this page you will see that
it contains no content except a reference to the CSS stylesheets, and the pre-built
Javascript app. Both of which are loaded up from static assets under `./client/build`.

The Javascript client code is under `./client/src` and the Webpack config file for building
it is `./webpack.config.js`. It can be built simply with `webpack -d`, which compiles the JS
and puts out a build called `bundle.js` under `./client/build`

All requests to Nginx of the form `/public/*` are aliased to `./client/build/*`.


Installation and Setup
-----------------------

The README instructions are fairly thorough and should get you through everything except
the installation of the ELK stack. The ELK stack was built using the tutorial mentioned
above and then modified to allow ingesting of Nginx logs. The config files for all the
elements of the ELK stack as well as the main stack are all stored under the `./wsgi`
folder within their appropriate sub-folders.

The requirements file used for the initial Python setup was `./requirements/requirements-sqlite.txt`.


Live Demo
----------

The main site: `http://162.243.145.163/`

The Kibana dashboard: `http://162.243.145.163:5601/`


Visualization 
--------------

Conversion funnel from initial view of the page to informative:

`http://162.243.145.163:5601/app/kibana#/visualize/edit/Informative-Conversion-Funnel?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-4h,mode:quick,to:now))&_a=(filters:!(),linked:!f,query:(query_string:(analyze_wildcard:!t,query:'url:%20%5C%2Fapi%5C%2Fv1%5C%2Fevent%5C%2Finitial%5C%2F*%20OR%20url:%20%5C%2Fapi%5C%2Fv1%5C%2Fevent%5C%2Finformative%5C%2F*')),uiState:(),vis:(aggs:!((id:'1',params:(),schema:metric,type:count),(id:'2',params:(customInterval:'2h',extended_bounds:(),field:'@timestamp',interval:m,min_doc_count:1),schema:segment,type:date_histogram),(id:'3',params:(filters:!((input:(query:(query_string:(analyze_wildcard:!t,query:'url:%20%5C%2Fapi%5C%2Fv1%5C%2Fevent%5C%2Finitial%5C%2F*'))),label:Initial),(input:(query:(query_string:(analyze_wildcard:!t,query:'url:%20%5C%2Fapi%5C%2Fv1%5C%2Fevent%5C%2Finformative%5C%2F*'))),label:Informative))),schema:group,type:filters)),listeners:(),params:(addLegend:!t,addTimeMarker:!f,addTooltip:!t,defaultYExtents:!f,mode:stacked,scale:linear,setYExtents:!f,shareYAxis:!t,times:!(),yAxis:()),title:'Informative%20Conversion%20Funnel',type:histogram))`

Dwell Time for a particular visitor:

`http://162.243.145.163:5601/app/kibana?#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:'2018-08-08T20:30:36.833Z',mode:absolute,to:'2018-08-08T21:40:00.000Z'))&_a=(columns:!(_source),index:'filebeat-*',interval:s,query:(query_string:(analyze_wildcard:!t,query:'url:%20%5C%2Fapi%5C%2Fv1%5C%2Fevent%5C%2F97c250c3-d0d3-48cb-84bd-3d90081742fc')),sort:!('@timestamp',desc))`


For both visualizations you may need to expand the search window (of time) int he top right corner, to the past 24 hours or so,
then zoom in on the part of the graph you are interested in viewing up close.

The visualizations are not the best. This is largely due to lack of time and my imperfect knowledge of Kibana.
Given more time and dedication I am sure that I could build a decent visualization either on top of Kibana or
by interfacing directly with Elasticsearch.



Tradeoffs
----------

I opted for a lot of pre-built tools as this sped up development time, and allowed me to address most or all of the
design points.

However, the downside is that I did not really architect any data model. And I was not able to build custom visualizations
which may hav e been more meaningful.

While I initially thought that logging events from Nginx was a quick and dirty hack, I feel that this offers various
unique advantages. The biggest of which is the ability to use Caching to scale, while not losing our ability to log
the events in question.

I was very lax with security, mostly so that it would be easy for me to get everything working together.
